def randfor(y_test):
    regr = RandomForestRegressor()
    regr.fit(np.array(X_train),np.array(y_train))
    y_pred = regr.predict(X_test)
    y_pred = pd.Series(y_pred.astype('int64'))
    y_pred.index = y_test.index
    y_test = y_test.astype('int64')
    diff = y_test - y_pred
    return diff
    
    #tuning RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

RFR_tun = GridSearchCV(regr, param_grid=random_grid)

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, y_train)
#rf_random.best_params_
y_pred_tun = pd.Series(rf_random.predict(X_test).astype('int64'))
y_pred_tun.index = y_test.index

model = xgb.XGBRegressor(colsample_bytree=0.5,
                 gamma=0,                 
                 learning_rate=0.001,
                 max_depth=3,
                 min_child_weight=1.5,
                 n_estimators=10000,                                                                    
                 reg_alpha=0.75,
                 reg_lambda=0.45,
                 subsample=0.8,
                 seed=42) 

model = model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# create dataframe
fb_df = df.copy()*-1

fb_df = fb_df.rename(columns={"ISTGemeinkosten" : "y"})
fb_df_cum = fb_df.groupby(fb_df.index.year).cumsum()

fb_df['ds'] = fb_df.index
fb_df_cum['ds'] = fb_df.index
fb_df_cum

regr_st= 2
regr_end = 4
regressoren = []

prophet = Prophet(growth='linear')
for i in range(regr_st,regr_end+1,1):
    prophet.add_regressor(fb_df.columns[i])
    regressoren.append(fb_df.columns[i])
    print('Regressor ', fb_df.columns[i], ' added')

start = '2017-01-01'
te_start = '2019-05-01'
te_end = '2019-12-01'
cols = ['y', 'ds'] + regressoren

fb_df_train = fb_df_cum[(fb_df_cum.index >= start) & (fb_df_cum.index < te_start)]
fb_df_train = fb_df_train[cols]
fb_df_train.reset_index(inplace=True)
del fb_df_train['Period']
fb_df_train

prophet.fit(fb_df_train)

# create future dataframe
future = prophet.make_future_dataframe(periods=8, freq='MS', include_history = True)

#for i in range(regr_st,regr_end+1,1):
#    future[fb_df.columns[i]] = fb_df[fb_df.columns[i]][start:te_end].values
future = future.merge(fb_df_cum[cols], on='ds')
del future['y']
future

forecast = prophet.predict(future)
y_pred = forecast['yhat'][forecast['ds'] >= te_start]
y_true = fb_df_cum['y'][te_start:te_end]

from fbprophet.plot import add_changepoints_to_plot
fig = prophet.plot(forecast)
a = add_changepoints_to_plot(fig.gca(), prophet, forecast)
